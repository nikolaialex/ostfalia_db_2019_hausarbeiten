# Kubernetes

Die Idee von Kubernetes ist die Entkopplung der Container von ihrer Serviceadresse, was durch eine Serviceschicht umgesetzt ist. Somit kann jede Serviceanfrage an einen Service gestellt und von mehreren Containern beantwortet werden. Jeder Service ist somit horizontal skalierbar. Kubernetes kümmert sich neben der Hochverfügbarkeit und des Routings von Services hauptsächlich um den Betrieb von Containern über einen längeren Zeitraum. Der automatisierte Neustart eines Pods ist dabei zentrales Instrument zur Skalierung.

## Infrastruktur

Aufgesetzt auf dem Betriebssystem läuft Kubernetes.
Kubernetes kann als ein in Schichten aufgebautes System dargestellt werden, wobei jede höhere Schicht die Komplexität der niedrigeren Ebenen abstrahiert. Kubernetes fasst einzelne physische oder virtuelle Maschinen zu einem Cluster zusammen und verwendet ein gemeinsames Netzwerk für die Kommunikation zwischen den einzelnen Servern. Dieser Cluster ist die physische Plattform, auf der alle Kubernetes-Komponenten, -Funktionen und -Arbeitsauslastungen konfiguriert werden.

Anwendungen und Dienste selbst werden auf dem Cluster in Containern ausgeführt. Die zugrunde liegenden Komponenten stellen sicher, dass der gewünschte Status der Anwendungen mit dem tatsächlichen Status des Clusters übereinstimmt. Benutzer interagieren mit dem Cluster, indem sie entweder direkt oder mit Clients und Bibliotheken mit dem Haupt-API-Server kommunizieren. Um eine Anwendung oder einen Dienst zu starten, wird in JSON oder YAML ein deklarativer Plan übermittelt, der definiert, was erstellt und wie es verwaltet werden soll. Der Kubernetes-Master erstellt dann den Plan und ermittelt, wie er in der Infrastruktur ausgeführt wird, indem er die Anforderungen und den aktuellen Status des Systems überprüft. Diese Gruppe von benutzerdefinierten Anwendungen, die nach einem festgelegten Plan ausgeführt werden, bilden die letzte Schicht von Kubernetes.

## Komponenten

Das Kubernetes-System besteht aus mehreren Bestandteilen, die in diesem Abschnitt näher erläutert werden sollen:

Kubernetes-Master
Die Maschinen im Cluster erhalten jeweils eine Rolle innerhalb des Kubernetes-Ökosystems. Ein Server fungiert dabei als Kubernetes-Master. Dieser Server fungiert als Gateway und Gehirn für den Cluster, indem er eine API für Benutzer und Clients bereitstellt, den Zustand anderer Server überprüft, festlegt, wie die Arbeit am besten aufgeteilt und zugewiesen werden soll und die Kommunikation zwischen anderen Komponenten koordiniert. Der Master-Server fungiert als primärer Ansprechpartner für den Cluster und ist für den Großteil der zentralisierten Logik verantwortlich, die Kubernetes bereitstellt. Der Kubernetes-Master dient als Ansprechpartner für Administratoren und Benutzer.
Er besteht aus drei Prozessen, die auf einem einzelnen Node im Cluster ausgeführt werden, der als Master-Node bezeichnet wird. Diese Prozesse sind: kube-apiserver, kube-controller-manager und kube-scheduler.

kube-apiserver
Einer der wichtigsten Services des Kubernetes-Master ist ein API-Server. Dies ist der Hauptverwaltungspunkt des gesamten Clusters, da ein Benutzer die Workloads und Organisationseinheiten von Kubernetes konfigurieren kann. Er ist auch dafür verantwortlich, dass die Servicedetails der bereitgestellten Container übereinstimmen und fungiert als Verbindungsstück zwischen verschiedenen Komponenten, um den Clusterzustand aufrechtzuerhalten und Informationen und Befehle zu verbreiten.

Der API-Server implementiert eine RESTful-Schnittstelle, sodass viele verschiedene Tools und Bibliotheken problemlos mit ihm kommunizieren können. Ein Client namens kubectl ist als Standardmethode für die Interaktion mit dem Kubernetes-Cluster von einem lokalen Computer aus verfügbar.

kube-controller-manager
Der Controller Manager ist ein allgemeiner Dienst, der viele Aufgaben hat. Primär werden verschiedene Controller verwaltet, die den Status des Clusters steuern, die Workload-Lebenszyklen verwalten und Routineaufgaben ausführen. Ein Replikationscontroller stellt beispielsweise sicher, dass die Anzahl der Replikate, die für einen Pod definiert sind, mit der Anzahl übereinstimmt, die aktuell im Cluster bereitgestellt werden. Wenn eine Änderung festgestellt wird, liest die Steuerung die neuen Informationen und implementiert die Prozedur, die den gewünschten Zustand erfüllt. Dies kann das Vergrößern oder Verkleinern einer Anwendung oder das Anpassen von Endpunkten usw. beinhalten.

kube-scheduler
Der Prozess, der Workloads tatsächlich bestimmten Knoten im Cluster zuweist, ist der Scheduler. Dieser Service liest die Betriebsanforderungen eines Workloads ein, analysiert die aktuelle Infrastrukturumgebung und platziert die Arbeit auf einem akzeptablen Node. Der Scheduler ist dafür verantwortlich, die verfügbare Kapazität auf jedem Host zu verfolgen, um sicherzustellen, dass die Workloads nicht über die verfügbaren Ressourcen hinaus geplant werden. Der Scheduler muss die Gesamtkapazität sowie die Ressourcen kennen, die bereits den einzelnen Servern mittels der vorhandenen Workloads zugewiesen sind.

Kubernetes-Node
Die anderen Maschinen im Cluster werden als Kubernetes-Nodes bezeichnet: Server, die für die Annahme und Ausführung von Workloads unter Verwendung lokaler und externer Ressourcen verantwortlich sind. Um die Isolation, Verwaltung und Flexibilität zu verbessern, führt Kubernetes Anwendungen und Dienste in Containern aus. Daher muss jeder Node mit einer Container-Laufzeitumgebung (wie Docker oder rkt) ausgestattet sein. Der Knoten empfängt Arbeitsanweisungen vom Kubernetes-Master und erstellt oder zerstört Container entsprechend und passt die Netzwerkregeln an, um den Datenverkehr entsprechend weiterzuleiten.

Pods
Ein Pod ist eine Kubernetes-Abstraktion, die eine Gruppe von einem oder mehreren Anwendungscontainern und einigen gemeinsam genutzten Ressourcen für diese Container darstellt.
Pods werden von Kubernetes überwacht, um sie bei Bedarf neu zu starten oder ihnen Zeit zur Selbstheilung zu geben, falls der Pod ausgelastet ist. Der Neustart wird durch die Liveness-Probe ausgelöst und die Selbstheilung durch die Readiness-Probe. Beide Probes können entweder HTTP-Endpunkte oder Container-Kommandos sein. Sie werden nach der ersten initialisierten Zeit von Kubernetes in einem vorher definierbaren Takt abgefragt. 
Im wiederholten Fehlerfall passiert folgendes:
Liveness-Fehlerfall: Kubernetes started den Pod neu.
Readiness-Fehlerfall: Kubernetes routet keinen weiteren Traffic zu diesem Pod.

Kubernetes eignet sich bestens, um skalierbare, hochverfügbare Container-Workloads auf einer stark abstrahierten Plattform ausführen zu können. Die Leistungsfähigkeit, Flexibilität und Robustheit von Kubernetes ist in der Open-Source-Welt beispiellos. 

https://jaxenter.de/der-steuermann-fuers-containerschiff-65844 abgerufen am: 27.11.2019
https://www.redhat.com/de/topics/containers/what-is-kubernetes abgerufen am 27.12.2019
https://cloud.google.com/kubernetes-engine/docs/concepts/pod?hl=de abgerufen am: 27.12.2019
https://www.admin-magazin.de/Das-Heft/2018/06/Kubernetes-Storage-mit-Rook abgerufen am 27.12.2019
https://kubernetes.io/docs/concepts/containers/runtime-class/ abgerufen am: 05.12.2020
https://www.innoq.com/de/articles/2019/05/kubernetes-einstieg/ abgerufen am: 12.01.2020
https://blogs.itemis.com/de/kubernetes-wer-nicht-skaliert-verliert abgerufen am 12.01.2020
https://www.digitalocean.com/community/tutorials/an-introduction-to-kubernetes abgerufen am 14.01.2020


---

[<< Docker](2_docker.md) | [Inhaltsverzeichnis](0_inhalt.md) | [Run Databases in Kubernetes >>](4_dbInK8s.md)

---

## TODO

- Wie müssen die Host-Maschinen konditioniert sein, damit Kubernetes da laufen kann?
- Grafik von Kubernetes mit den Komponenten und deren Zusammenspiel
- Wie stellt Kubernetes fest ob Container bzw. Pods laufen und ready sind? => Health und Readiness Checks, wenn Check fehlschlägt, dann Neustart
- Monitoring von Host-Ressourcen => Wie entscheidet Kubernetes wie und wann Lastverteilung stattfinden soll?
- Tabelle für Literaturverzeichnis einfügen und Links mit Titel und Aufrufdatum ergänzen
- Besonderheiten von Kubernetes und welche Vorteile
